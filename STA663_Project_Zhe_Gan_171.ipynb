{
 "metadata": {
  "name": "",
  "signature": "sha256:96bb0bd59d2eda37dcb793bb758421ef218489c251c2a0b76de4fbe5370c2433"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Convolutional Factor Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There has been significant recent interest in multi-layered or \"deep\" models for representation of general data, with a particular focus on imagery. Popular deep learning models include: Deep Belief Network, Deep Boltzmann Machine, Deep Autoencoder, Deep Convolutional Neural Networks, etc. Among these, deep covolutional networks have demonstrated excellent performance on image classification and tasks. There are at least two key components of this model: 1) convolution operator, which considers all possible shifts of canonical filters; 2) a deep architecture, in which the features of a given layer serve as the inputs to the next layer above. \n",
      "\n",
      "However, traditionally, the training of a convolutional neural network is imposed as an optimization problem. In this course project, I will focus on developing Bayesian generative model for deep convolutional dictionary learning. The paper link is here: http://people.ee.duke.edu/~lcarin/Bo7.pdf. As stated in the paper, some advantages of the proposed model are: (i) the number of filters at each layer of the deep model is inferred from the data by an IBP/BP construction; (ii) multi-task feature learning is performed for simultaneous analysis of different families of images, using the hierarchical beta process (HBP); (iii) fast computations are performed using Gibbs sampling, where the convolution operation is exploited directly within the update\n",
      "equations; and (iv) sparseness is imposed on the filter coefficients and filters themselves, via a Bayesian generalization\n",
      "of the L1 regularizer. One possible disadvtange of this model is the inference, as the Gibbs sampling is typically slow, and difficult to scale up to deal with large-scale dataset. Therefore, how to make the inference scalable should be an interesting topic.\n",
      "\n",
      "Recently, I am focusing my research on designing efficient and scalable Bayesian inference algorithms for deep learning models. Therefore, the course project will be particularly helpful for my research. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}