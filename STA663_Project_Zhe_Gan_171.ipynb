{
 "metadata": {
  "name": "",
  "signature": "sha256:62b11283b0482cf570833088771317ad773da2897b3add745ef61cc745096f06"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Convolutional Factor Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There has been significant recent interest in multi-layered or \"deep\" models for representation of general data, with a particular focus on imagery. Popular deep learning models include: Deep Belief Network, Deep Boltzmann Machine, Deep Autoencoder, Deep Convolutional Neural Networks, etc. Among these, deep covolutional networks have demonstrated excellent performance on image classification tasks. There are at least two key components of this model: 1) convolution operator, which considers all possible shifts of canonical filters; 2) a deep architecture, in which the features of a given layer serve as the inputs to the next layer above. \n",
      "\n",
      "However, traditionally, the training of a convolutional neural network is imposed as an optimization problem. In this course project, I will focus on developing Bayesian generative model for deep convolutional dictionary learning. The paper link is here: http://people.ee.duke.edu/~lcarin/Bo7.pdf. As stated in the paper, some advantages of the proposed model are: (i) the number of filters at each layer of the deep model is inferred from the data by an IBP/BP construction; (ii) multi-task feature learning is performed for simultaneous analysis of different families of images, using the hierarchical beta process (HBP); (iii) fast computations are performed using Gibbs sampling, where the convolution operation is exploited directly within the update\n",
      "equations; and (iv) sparseness is imposed on the filter coefficients and filters themselves, via a Bayesian generalization\n",
      "of the L1 regularizer. One possible disadvtange of this model is the inference, as the Gibbs sampling is typically slow, and difficult to scale up to deal with large-scale dataset. Therefore, how to make the inference scalable should be an interesting topic.\n",
      "\n",
      "Recently, I am focusing my research on designing efficient and scalable Bayesian inference algorithms for deep learning models. Therefore, the course project will be particularly helpful for my research. \n",
      "\n",
      "The Github repository can be found at https://github.com/zhegan27/sta-663-zhe-gan.git.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model Formulation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The proposed model is applicable to general data, for which a convolutional dictionary representation is appropriate. One may, for example, apply the model to one-dimensional signals such as audio, or to two-dimensional imagery. In this project, we focus on imagery, and hence assume two-dimensional signals and convolutions. \n",
      "\n",
      "Assume $N$ gray-scale images $\\{X^{(n)}\\}_{n=1,\\ldots,N}$, with $X^{(n)} \\in R^{N_x \\times N_y}$; the images are analyzed jointly to learn the convolutional dictionary $\\{D^{(k)}\\}_{k=1,\\ldots,K}$. Specifically, consder the model\n",
      "\n",
      "\\begin{align}\n",
      "X^{(n)} = \\sum_{k=1}^K D^{(k)} \\ast (Z^{(n,k)} \\odot W^{(n,k)}) + E^{(n)},\n",
      "\\end{align}\n",
      "\n",
      "where $\\ast$ is the convolution operator, $\\odot$ denotes the Hadamard (element-wise) product, the elements of $Z^{(n,k)}$ are in $\\{0,1\\}$, the elements of $W^{(n,k)}$ are real, and $E^{(n)}$ represents the residual. Considering $D^{(k)} \\in R^{n_{d_x} \\times n_{d_y}}$ (typically $n_{d_x} \\ll N_x$ and $n_{d_y} \\ll N_y$), the corresponding weights $ Z^{(n,k)} \\odot W^{(n,k)}$ are of size $(N_x - n_{d_x}+1) \\times (N_y - n_{d_y}+1)$.\n",
      "\n",
      "Let $w_{i,j}^{(n,k)}$ and $z_{i,j}^{(n,k)}$ represent elements $(i,j)$ of $W^{(n,k)}$ and $Z^{(n,k)}$, respectively. Within a Bayesian construction, the priors for the model may be represented as \n",
      "\n",
      "\\begin{align}\n",
      "z_{i,j}^{(n,k)} &\\sim \\mbox{Bernoulli} (\\pi_{i,j}^{(n,k)}), &  \\pi_{i,j}^{(n,k)} &\\sim \\mbox{Beta} (a_0,b_0), \\\\\n",
      "w_{i,j}^{(n,k)} &\\sim N (0,\\gamma_w^{-1}), & D^{(k)} &\\sim N(0,\\gamma_d^{-1} I), & E^{(n)} &\\sim N(0,\\gamma_{e} I), \\\\\n",
      "\\gamma_w &\\sim \\mbox{Ga} (a_w,b_w), & \\gamma_d &\\sim \\mbox{Ga}(a_d,b_d), & \\gamma_e &\\sim \\mbox{Ga}(a_e, b_e),\n",
      "\\end{align}\n",
      "\n",
      "where $i=1,\\ldots,N_x - n_{d_x}+1; j=1,\\ldots,N_y - n_{d_y}+1$, $\\mbox{Ga}(\\cdot)$ denotes the gamma distribution. $I$ represents the identity matrix, and $\\{a_0,b_0,a_w,b_w, a_d,b_d, a_em b_e\\}$ are hyperparameters. While the model may look somewhat complicated, local conjugacy admits Gibbs sampling or variational Bayes inference."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Posterior Inference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The local conditional posterior distribution for all parameters of the model is manifested in closed form, yielding efficient Gibbs sampling algorithm. The FFT is leveraged to accelerate computation of the covolution operations.\n",
      "\n",
      "For each MCMC iteration, the samples are drawn from:\n",
      "\n",
      "For $D^{(k)}$: we have $D^{(k)} \\sim N(\\mu^{(k)},\\Sigma^{k})$, where\n",
      "\n",
      "I am still deriving the update equations. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Experiments"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will do experiments on the MNIST dataset. For comparison, we can compare the classification results using the traditional convolutinal neural network. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}