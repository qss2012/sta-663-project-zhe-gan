\documentclass{article} % For LaTeX2e
\usepackage{CJK}
\usepackage{project,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{booktabs}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

\newcommand{\beq}{\vspace{0mm}\begin{equation}}
\newcommand{\eeq}{\vspace{0mm}\end{equation}}
\newcommand{\beqs}{\vspace{0mm}\begin{eqnarray}}
\newcommand{\eeqs}{\vspace{0mm}\end{eqnarray}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}
\newcommand{\Amat}[0]{{{\bf A}}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}[0]{{{\bf E}}}
\newcommand{\Fmat}[0]{{{\bf F}}\xspace}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}[0]{{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{{{\bf L}}}
%\newcommand{\Mmat}[0]{{{\bf M}}\xspace}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}[0]{{{\bf N}}\xspace}
\newcommand{\Omat}[0]{{{\bf O}}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}[0]{{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{{{\bf R}}}
\newcommand{\Smat}[0]{{{\bf S}}}
\newcommand{\Tmat}[0]{{{\bf T}}}
\newcommand{\Umat}[0]{{{\bf U}}}
\newcommand{\Vmat}[0]{{{\bf V}}}
\newcommand{\Wmat}[0]{{{\bf W}}}
\newcommand{\Xmat}[0]{{{\bf X}}}
\newcommand{\Ymat}{{\bf Y}}
%\newcommand{\Ymat}[0]{{{\bf Z}}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\av}[0]{{\boldsymbol{a}}}
\newcommand{\bv}[0]{{\boldsymbol{b}}}
\newcommand{\cv}[0]{{\boldsymbol{c}}}
\newcommand{\dv}{\boldsymbol{d}}
\newcommand{\ev}[0]{{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{{\boldsymbol{f}}\xspace}
\newcommand{\gv}[0]{{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{{\boldsymbol{h}}}
\newcommand{\iv}[0]{{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{{\boldsymbol{l}}}
\newcommand{\mv}[0]{{\boldsymbol{m}}}
\newcommand{\nv}[0]{{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{{\boldsymbol{p}}}
\newcommand{\qv}[0]{{\boldsymbol{q}}\xspace}
\newcommand{\rv}{\boldsymbol{r}}
\newcommand{\sv}[0]{{\boldsymbol{s}}}
\newcommand{\tv}[0]{{\boldsymbol{t}}\xspace}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\wv}{\boldsymbol{w}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
\newcommand{\cdotv}{\boldsymbol{\cdot}}

\newcommand{\Gammamat}[0]{{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}{\boldsymbol{\Theta}}
\newcommand{\Betamat}{\boldsymbol{\Beta}}
\newcommand{\Lambdamat}{\boldsymbol{\Lambda}}
\newcommand{\Ximat}[0]{{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{{\boldsymbol{\Sigma}}}
\newcommand{\Upsilonmat}[0]{{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\Psimat}{\boldsymbol{\Psi}}
\newcommand{\Omegamat}[0]{{\boldsymbol{\Omega}}}

\newcommand{\alphav}{\boldsymbol{\alpha}}
\newcommand{\betav}[0]{{\boldsymbol{\beta}}}
\newcommand{\gammav}[0]{{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}
\newcommand{\zetav}[0]{{\boldsymbol{\zeta}}\xspace}
\newcommand{\etav}[0]{{\boldsymbol{\eta}}\xspace}
\newcommand{\ellv}[0]{{\boldsymbol{\ell}}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\iotav}[0]{{\boldsymbol{\iota}}}
\newcommand{\kappav}[0]{{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{{\boldsymbol{\lambda}}}
\newcommand{\muv}[0]{{\boldsymbol{\mu}}}
\newcommand{\nuv}[0]{{\boldsymbol{\nu}}}
\newcommand{\xiv}[0]{{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}{\boldsymbol{\pi}}
\newcommand{\rhov}[0]{{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{{\boldsymbol{\sigma}}\xspace}
\newcommand{\tauv}[0]{{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\chiv}[0]{{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\varthetav}{\boldsymbol{\vartheta}}
\newcommand{\omegav}[0]{{\boldsymbol{\omega}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\NNcal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Gcal}{\mathcal{G}}

\title{Bayesian Sparse Infinite Factor Model \\ for Gene Expression Analysis}

\author{
Zhe Gan(zg27) \\
Department of ECE, \ Duke University \\
\texttt{zhe.gan@duke.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy

\begin{document}

\bibliographystyle{plain}
\maketitle

\begin{abstract}
This paper presents a non-parametric Bayesian framework for gene expression analysis. The multiplicative gamma process is imposed to infer the number of latent factors, while the Dirichlet process mixture model is employed to model the nonlinearity of the latent low dimensional subspace. When side information is available, a max-margin classifier is trained to do the classification using the inferred latent factors. A Gibbs sampler is developed to do the posterior inference. Experimental results on gene data from recent viral challenge-studies demonstrate that the proposed model can achieve good performance.
\end{abstract}

\section{Introduction}
When performing gene expression analysis for inference of relationships between genes and conditions/phenotypes, one typically must analyze a small number of gene samples, each composed of expression values from tens of thousands of genes. In this setting, the observed data is $\Xmat \in \R^{p \times n}$, each column of which corresponds to one of the $n$ samples, containing the associated expression values of all $p$ genes under investigation \cite{chen2010bayesian}.

We typically has to address the ``large $p$, small $n$'' problem \cite{bernardo2003bayesian}, in which $n \ll p$. Direct analysis is infeasible. In order to yield a reliable inference, two assumptions are widely employed: 1) only a small number of latent factors are important for prediction; and 2) only a small number of genes are responsible for each latent factor. Therefore, the gene expression analysis problem converts to a sparse factor analysis problem, which has been intensively studied in the machine learning community.

In this setting, the model can be expressed as $ \Xmat = \Amat \Smat + \Emat $,
where $\Amat \in \R^{p \times r}$ refers to the factor loadings, $\Smat \in \R^{r\times n}$ refers to the factor scores, and $\Emat$ represents the noise term \cite{carvalho2008high}. To address the problem that $n \ll p$, a sparseness constraint is usually imposed on the columns of $\Amat$, with the idea that each column of $\Amat$ corresponds to a biological ``pathway'', defined by a few correlated genes. Using a Bayesian framework, the sparseness constraint can be specified by using a spike-and-slab prior \cite{bernardo2003bayesian} \cite{carvalho2008high}, or other shrinkage priors, such as the Student-t prior \cite{tipping2001sparse}, the horseshoe prior \cite{carvalho2009handling} and the generalized beta mixture of Gaussian prior \cite{armagan2011generalized}. To impose sparseness, we will use the spike-and-slab prior , since this is not a major concern of this paper.

However, one of the most important drawbacks of the above model is the assumption of a known prior knowledge of the number of latent factors. Cross validation can be implemented to find the optimal value that fits the data. A more principled way to avoid this problem is to use non-parametric Bayesian methods to automatically infer the number of factors. An example of recent research in this direction employs the Indian buffet process (IBP) \cite{griffiths2005infinite}, of which the de Finetti mixing distribution is called the Beta process (BP) \cite{thibaux2007hierarchical} \cite{paisley2009nonparametric}. In this paper, we utilize the multiplicative gamma process (MGP) \cite{bhattacharya2011sparse} to infer the number of factors, with the merit of easy implementation and good performance.

Another issue considered in this paper is that the original signal might lie in a nonlinear manifold. This nonlinearity can be modeled by using the Gaussian mixture model. In order to infer the number of mixture components, the Dirichlet process (DP) mixture model is utilized, coupled with the multiplicative gamma process to jointly infer the number of mixture components and the number of latent factors \cite{chen2010compressive}. In this paper, the truncated stick breaking construction is utilised to represent the Dirichlet process \cite{ishwaran2001gibbs}, which results in an easy incorporation with the other parts of the whole model.

Finally, when the class label of the gene data is available, one can further do the classification using the inferred latent factors. Many classification techniques can be considered, such as logistic regression and probit model. In this paper, we train the classifier in a max-margin manner by using Bayesian Support Vector Machines (SVM) \cite{polson2011data}.  The classifier can be further incorporated into the factor analysis model to improve the discriminative power of the model, which is a future direction of this paper. This supervised factor analysis model has already been considered in the dictionary learning problem \cite{babagholami2013bayesian}, where the sparse codes, the dictionary and the classifier are learnt jointly.

Combining all the elements, this paper presents a non-parametric Bayesian framework for sparse infinite factor analysis. The possible contributions of this paper are summarized as follows: 1) the multiplicative gamma process prior is imposed to automatically infer the number of latent factors; 2) the Dirichlet process mixture model is employed to learn the nonlinearity of the inferred factor scores; 3) a max-margin classifier is trained to do the classification based on the inferred factors.

The remainder of this paper is organized as follows: Section 2 introduces the background. Section 3 introduces the proposed model.  Section 4 presents the experimental results. Section 5 discusses future work. The appendix demonstrates the posterior inference using Gibbs sampling.


\section{Background}

\subsection{Multiplicative Gamma Process}
Consider a factor model of form $\xv_i = \Amat \sv_i + \epsilon_i$, $\epsilon_i \sim \Ncal (\epsilon_i|0,\Lambdamat^{-1})$, where $\Amat = \{ A_{jk}, 1 \leq j \leq p, 1 \leq k \leq K \}$ and $\xv_i \in \R^p, \sv_i \in \R^K$. The multiplicative gamma process is defined on each $A_{jk}$ as \cite{bhattacharya2011sparse}
\beqs A_{jk} \sim \Ncal(A_{jk}|0,\phi_{jk}^{-1}\tau_k^{-1}), \phi_{jk} \sim \mbox{Gamma} (\phi_{jk}|3/2,3/2) \\
\tau_k = \prod_{l=1}^k \delta_l, \delta_1 \sim \mbox{Gamma} (\delta_1|a_1,1), \delta_l \sim \mbox{Gamma} (\delta_l|a_2,1), l \geq 2 \eeqs
where $\delta_l, l=1,\ldots,\infty$ are independent. The $\tau_k$ is a globally shared shrinkage parameter for factor loadings $A_k$, and $\phi_{jk}$ is a local shrinkage parameter for $A_{jk}$. The $\prod_{l=1}^k \delta_l$ term are stochastically increasing under the restriction $a_2 >1$, which favors more shrinkage as $n$ increases.

\subsection{Dirichlet Process}
The Dirichlet process is a stochastic process used in Bayesian non-parametric models of data, particularly in Dirichlet process mixture models. It is a standard method to cluster data in a potentially infinite number of clusters. In this paper, we mainly consider the stick breaking construction of the Dirichlet process. It is simply given as follows \cite{teh2010dirichlet}:
\beqs \beta_k \sim \mbox{Beta} (1,\alpha), \pi_k = \beta_k \prod_{l=1}^{k-1} (1-\beta_l) \\
\theta_k^* \sim H, G = \sum_{k=1}^\infty \pi_k \delta_{\theta_k^*}
\eeqs
Then $G \sim \mbox{DP} (\alpha, H)$, with base distribution $H$ and concentration parameter $\alpha$.The construction of $\pi$ can be understood metaphorically as follows. Starting with a stick of length $1$, we break it at $\beta_1$, assigning $\pi_1$ to be the length of stick we just broke off. Now recursively break the other portion to obtain $\pi_2, \pi_3$ and so forth \cite{teh2010dirichlet}. We use this stick breaking construction to implement the DP mixture model.

\subsection{Data Augmentation for Support Vector Machines}
Consider the binary outcomes $\yv = \{y_i \in \{-1,1\}, i=1,\ldots,n\}$ based on the predictors $\Xmat = \{\xv_i = (1,x_1,\ldots,x_{K-1}), i=1,\ldots,n \}$, Support Vector Machine (SVM) aims to find a set of coefficients $\beta = \{\beta_k, k=1,\ldots,K\}$ that minimize the objective function
\beq d_{\alpha} (\beta,\nu) = \sum_{i=1}^n \max (1-y_i\xv_i^T \beta,0)+\nu^{-\alpha} \sum_{j=1}^K |\beta_j/\sigma_j|^{\alpha} \label{equation1} \eeq
where $\sigma_j$ is the standard deviation of the $j$'th element of $\xv$ and $\nu$ is a tuning parameter.

SVM utilizes the optimization technique to solve the above problem. However, minimizing Equation (\ref{equation1}) is equivalent to finding the mode of the pseudo-posterior distribution $p(\beta|\nu,\alpha,\yv)$ defined by
\beq p(\beta|\nu,\alpha,\yv) \propto L(\yv|\beta) p(\beta|\nu,\alpha) \eeq
The data dependent factor $L(y|\beta)$ is a pseudo-likelihood
\beq L(y|\beta) = \prod_{i} L_i(y_i|\beta) = \exp \left\{ -2 \sum_{i=1}^n \max (1-y_i \xv_i^T \beta,0) \right\} \eeq
which can be expressed as a location-scale mixture of normals by using data augmentation techniques \cite{polson2011data}. To be specific,
\beqs L_i(y_i|\beta) &=& \exp \{ -2 \max (1-y_i \xv_i^T \beta,0) \} \\
&=& \int_0^\infty \frac{1}{\sqrt{2\pi \lambda_i}} \exp \left( -\frac{1}{2} \frac{(1+\lambda_i-y_i\xv_i^T\beta)^2}{\lambda_i} \right) \eeqs
Therefore, the SVM optimization problem can be converted into a statistical inference problem with full conjugacy. No relaxation technique is used to approximate the original goal function. Both Gibbs sampling and variational inference can be developed to do the posterior inference.

\section{Model}
The gene data can be modeled as $ \Xmat = \Amat \Lambdamat \Smat + \Emat $, where $\Xmat \in \R^{p\times n}$ is the data matrix,$\Amat \in \R^{p\times K}$ is the factor loadings, $\Lambdamat \in \R^{K \times K}$ is a diagonal matrix which plays the same role as the singular value matrix in the SVD decomposition, $\Smat \in \R^{K\times n}$ is the factor scores and $\Emat \in \R^{p \times n}$ is the noise term. Here, $p$ is the dimension of genes, $n$ is the sample size, $K$ is the number of latent factors. We impose the spike-and-slab prior on $\Amat$, the multiplicative gamma process prior on $\Lambdamat$ and the Dirichlet process mixture of Gaussian prior on $\Smat$. The truncation level of the DP mixture is set to be $T$. Therefore, the complete model is expressed as
\begin{align}
 \xv_i &\sim \Ncal \left( \Amat \Lambdamat \sv_i, \mbox{diag}(\psi_1^{-1},\ldots,\psi_p^{-1}) \right)   &   \psi_j &\sim \mbox{Gamma}(e,f)   \\
A_{jk} &= \pi_j \Ncal (0, 1) + (1-\pi_j)\delta_0   &  \pi_j &\sim \mbox{Beta} (a, b) \\
\Lambdamat &= \mbox{diag}(\lambda_1,\ldots,\lambda_K)   &  \lambda_k &\sim \Ncal (0,\tau_k^{-1}) \,\,\ \tau_k = \prod_{l=1}^k \delta_l \\
\delta_1 &\sim\mbox{Gamma}(a_1,1)   &  \delta_l &\sim \mbox{Gamma}(a_2,1) \,\ \mbox{for} \,\ l\geq 2 \\
\sv_i &\sim \prod_{t=1}^T \left[ \Ncal (\sv_i|\muv_t,\Imat_K) \right]^{I(z_i = t)}   &  \muv_t &\sim \Ncal (0,\gamma_0^{-1} \Imat_K)  \\
z_i &\sim \prod_{t=1}^T \left[ \rho_t \right]^{I(z_i = t)}   &  \rho_t &= \nu_t \prod_{l=1}^{t-1} (1-\nu_l) \\
\nu_l &\sim \mbox{Beta} (1,\eta) \,\,\ \mbox{for} \,\,\ l \in [1,T-1] \,\,\ v_T = 1   &  \eta &\sim \mbox{Gamma} (c,d)
\end{align}
where $i=1,\ldots,n; \ j=1,\ldots,p; \ k = 1,\ldots,K; \ t = 1,\ldots,T$.

When side information is available, we can use the inferred latent factors to do the classification in a max-margin manner. Define $\Hmat = [1_{1 \times n}; \Smat]$, The Bayesian SVM can be formulated as the following:

\beqs
1-y_i^{c}\hv_i^T \beta^{c} &\sim& \Ncal (-\gamma_i^{c},\gamma_i^{c})  \\
\beta^{c} &\sim& \Ncal(0,\mbox{diag}(\omega_1^{c},\ldots,\omega_K^{c})) \\
\omega_k^{c} &\sim& \mbox{Exponential} (2)
\eeqs
Since the Bayesian SVM is a binary linear classifier, the one-versus-others strategy is employed to do the multiclass classification. The subscript $c$ represents the corresponding multiple one-versus-others cases.


\section{Experimental Results}
In this section, we present the performance of the proposed model on gene expression analysis. The gene-expression data were acquired in the recent viral challenge-studies, which were executed after receiving institutional review board (IRB) approval from Duke University. Three labels are provided, which are ``viral'', ``bacterial'' and ``non-infectious fever''. In this paper, $p=1,000, n=280, K=50, T=50$. Note that the original number of genes under investigation is $5,886$, we select the important $1,000$ genes to speed up the Gibbs sampling.

The Gibbs sampler is initialized randomly. After $2,000$ burnins, we collect $1,000$ posterior samples. From Figure (\ref{figure1}) we can see that 28 factors and 3 mixture components are inferred. A closer observation will show that the right part of $\Amat$ appears smooth, while the bottom part of $\Smat$ appears rough, which can be interpreted as the following. First, we impose a simple spike-and-slab prior on $\Amat$, therefore the MGP shrinkage prior on $\Lambdamat$ can also shrink the corresponding part of $\Amat$ to zero. However, we impose a more complex DP mixture structure on $\Smat$, therefore the rough bottom part of $\Smat$ attempts to model the noise to decrease the mean square error.
\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics [width=5in]{inferred_matrix.png}  % uncomment this line and put the figure in the same folder as this document.
   \caption{the Inferred Matrix and the Inferred Number of Latent Factors and Mixture Components} \label{figure1}
\end{figure}

The difference between the original data matrix and the inferred matrix can be seen in Figure (\ref{figure2}). As can be seen, our proposed model can capture the structure of the gene data matrix.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\hline
name & accuracy (no DP) & accuracy (DP mixture)  \\
\hline
viral vs. others & 85.00\% & 90.71 \% \\
\hline
bacterial vs. others & 82.50\% & 90.36 \% \\
\hline
non-infectious fever vs. others & 84.50\% & 85.80 \%  \\
\hline
\end{tabular}
\caption{Leave-one-out cross validation (averaged on 4 runs)} \label{table1}
\end{table}

In order to evaluate the performance of the classifier using the inferred factors, we implement the leave-one-out cross validation. To illustrate the advantage of the proposed model, we also implement the model without the DP mixture prior on $\Smat$. The results are shown in Table (\ref{table1}). As can seen from the table, the proposed model can achieve very good performance.
\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics [width=4in]{difference.png}  % uncomment this line and put the figure in the same folder as this document.
   \caption{the Difference between original data matrix and the inferred matrix} \label{figure2}
\end{figure}

\section{Conclusion}

This paper presents a Bayesian sparse factor model. By using Bayesian non-parametric methods, the model has the advantage that can handle the potentially infinite latent factors and infinite number of mixture components on factor scores. When side information is available, a max-margin classifier is trained to do the classification. Experimental results on gene data show that the proposed model can achieve very competitve performance.  Some future extensions are listed as follows: 1) The factor scores are modeled nonlinearly using the DP mixture model, however in the classification part, a linear classifier is utilized, therefore a natural extension is to use a nonlinear max-margin classifier; 2) The factor model and the classifier can be coupled together to jointly learn the factor scores and the coefficients of the classifier, which is expected to further improve the performance; 3) variational inference can be developed to make the proposed model scalable to larger datasets.

\section*{Appendix}
Gibbs sampling is developed to do the posterior inference. We use the notation $p(\cdot|-)$ to represent the conditional distribution of the target variable given all the other variables fixed. Define $ X_{ji}^{-k} = x_{ji} - \sum_{l\not= k} A_{jl}\lambda_{l}s_{li},  \Psi = \mbox{diag}(\psi_1,\ldots,\psi_p), \Dmat = \Amat \Lambdamat$. For the factor analysis model, the updates of the Gibbs sampling are expressed as the following.

(1) Sample $\sv_i|- \sim \Ncal(\mu_i, \Sigma_i)$, where $\Sigma_i = (\Dmat^T\Psi \Dmat + \Imat_K)^{-1}, \mu_i =
\Sigma_i (\Dmat^T \Psi \xv_i + \muv_{z(i)})$;

(2) Sample $A_{jk}|- \sim \hat{\pi}_{jk} \Ncal(\mu_{jk}, \Sigma_{jk}) + (1-\hat{\pi}_{jk})\delta_0$, where
$ \sigma_{jk}^2 = \left( \psi_j \lambda_k^2 \sum_{i=1}^n s_{ki}^2 + 1 \right)^{-1} ,
\mu_{jk} = \sigma_{jk}^2 \left( \psi_j \lambda_k \sum_{i=1}^n s_{ki} X_{ji}^{-k}  \right),
\hat{\pi}_{jk} = \frac{\pi_j \sigma_{jk} \exp\left( \frac{\mu_{jk}^2}{2\sigma_{jk}^2} \right)}{\pi_j \sigma_{jk} \exp\left( \frac{\mu_{jk}^2}{2\sigma_{jk}^2} \right)+ (1-\pi_j)} $;

(3) Sample $\pi_j|- \sim \mbox{Beta} (\hat{a}_j, \hat{b}_j)$, where
$ \hat{a}_j = K- \sum_{k=1}^K I(A_{jk}=0) + a $,
$\hat{b}_j = \sum_{k=1}^K I(A_{jk}=0) + b $;

(4) Sample $\psi_j|- \sim \mbox{Gamma}(\hat{e}_j, \hat{f}_j)$,  where
$ \hat{e}_j = e+\frac{n}{2}$ , \\
$\hat{f}_j = f + \frac{1}{2} \sum_{i=1}^n \left( x_{ji} - \sum_{k=1}^K \lambda_k A_{jk} s_{ki} \right)^2
$;

(5) Sample $\lambda_k|- \sim \Ncal(\mu_k, \Sigma_k) $,where
$ \Sigma_k =\left( \sum_{i=1}^n \sum_{j=1}^p \psi_j A_{jk}^2 s_{ki}^2 + \tau_k \right)^{-1}$,\\
$\mu_k = \Sigma_k \left( \sum_{i=1}^n \sum_{j=1}^p \psi_j A_{jk} s_{ki} X_{ji}^{-k} \right)
$;

(6) Sample $\delta_1|- \sim \mbox{Gamma}(\hat{a}_1,\hat{b}_1)$, where
$ \hat{a}_1 = a_1 + \frac{K}{2} ,
\hat{b}_1 = 1+ \frac{1}{2}\sum_{k=1}^K \lambda_k^2 \tau_k^{(1)}
$;

(7) Sample $\delta_h|- \sim \mbox{Gamma}(\hat{a}_h,\hat{b}_h)$ for $h \geq 2$, where
$ \hat{a}_h = a_1 + \frac{K-h+1}{2},
\hat{b}_h =1+ \frac{1}{2}\sum_{k=h}^K \lambda_k^2 \tau_k^{(h)}
$, and
$ \tau_k^{(h)} = \prod_{l=1,l\not=h}^k \delta_l = \frac{\tau_k}{\delta_h} $;

(8) Sample $z_i|- \sim \mbox{Multinomial}(\hat{\rho}_{i})$, where $ p(z_i=t) = \frac{\exp(q_{it})}{\sum_{t=1}^T \exp(q_{it})} $,
and $q_t$ is defined as $ q_t = \ln \rho_t - \frac{1}{2} (\sv_i-\muv_t)^T (\sv_i-\muv_t)  $;


(9) Sample $\nu_t|- \sim \mbox{Beta} (\alpha_t, \beta_t)$, where
$
\alpha_t = 1+\sum_{i=1}^n I(z_i=t) ,
\beta_t = \eta + \sum_{i=1}^n I(z_t > t)
$;

(10) Sample $\eta|- \sim \mbox{Gamma} (\hat{c},\hat{d}) $, where
$
\hat{c} = c+T-1 ,
\hat{d} =d-\sum_{t=1}^{T-1} \log (1-\nu_t)
$;

(11) Define $n_t = \sum_{i=1}^n I (z_i=t) $, then sample $\muv_t|- \sim \Ncal (\mv_t, \sigma_t^2\Imat_K) $, where
$
\sigma_t^2 = (n_t+\gamma_0)^{-1} ,
\mv_t = \sigma_t^2 \left( \sum_{i=1}^n I(z_i = t)\sv_i \right)
$.

The Gibbs updates for the Bayesian SVM are the following. Here, we omit the script $c$ for simplicity.

(1) Define $\Gamma = \mbox{diag}(\gamma)$, $\Omega = \mbox{diag}(\omega)$, then sample $\beta|- \sim \Ncal(b,B)$, where \\
$ B = \left( \Hmat \Gamma^{-1} \Hmat^T + \Omega^{-1} \right)^{-1} ,
b = B \left( \Hmat (y \circ (1+\gamma^{-1})) \right) $

(2) Sample $\gamma_i^{-1}|- \sim \Ical \Gcal (|1-y_i \sv_i^T \beta|^{-1},1)$

(3) Sample $\omega_k^{-1}|- \sim \Ical \Gcal (\sqrt{2}|\beta|^{-1},2)$

\bibliography{project}

\end{document}
